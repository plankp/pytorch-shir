import torch

# Note: torch._dynamo.optimizations.training.aot_autograd got moved
from torch._dynamo.backends.common import (
  aot_module_simplified,
  fake_tensor_unsupported,  #  seems like it's not needed
)
from torch._decomp import core_aten_decompositions, get_decompositions
from torch.fx.passes.fake_tensor_prop import FakeTensorProp
from torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner
from torch.fx.passes.operator_support import OperatorSupport
from torch.fx.passes.tools_common import CALLABLE_NODE_OPS
from dataclasses import dataclass
from functorch.compile import make_boxed_func
import rewrite_pattern
import shir_type
import shir_lowering
from functools import reduce
from itertools import count

# some namespace aliases
aten = torch.ops.aten
prims = torch.ops.prims

# notes on decompositing core ATen to prims:
# -  not all core ATen ops have a prims decomposition (e.g. aten.mm)
# -  not all prims decompositions are "good" (e.g. aten.relu uses masking)
_decomps = get_decompositions([
  aten._to_copy,
  aten.sym_size,
  aten.add,
  aten.rsub,
  aten.sub,
  aten.mul,
  aten.dot,
  aten.sum,
  aten.where,
  aten.maximum,
  aten.squeeze,
  aten.expand.default,
  aten.permute.default,
  aten.unsqueeze.default,
])

def _tensor_to_matrix_csv(t: torch.Tensor, f):
  # decoding int64's is a bit annoying, ignore it!
  assert t.dtype in {torch.int8, torch.uint8, torch.int16, torch.int32}

  if t.ndim == 0:
    t = torch.unsqueeze(t, 0)
  if t.ndim == 1:
    t = torch.unsqueeze(t, 0)
  if t.ndim > 2:
    t = torch.flatten(t, 1)

  for row in t:
    d = row.shape[0]
    for i in range(d - 1):
      print(row[i].item(), ",", sep="", end="", file=f)
    print(row[d - 1].item(), file=f)

class SHIRGraphModule(torch.nn.Module):
  _iid_couter = count()   # assigns a number of each instance

  gm: torch.fx.GraphModule
  _inst_id: int
  _call_id: int
  _compiled: bool

  def __init__(self, gm: torch.fx.GraphModule):
    super(SHIRGraphModule, self).__init__()
    self.gm = gm
    self._compiled = False
    self._inst_id = next(self._iid_couter)
    self._call_id = 0

  def _get_input_names(self):
    names = []
    for n in self.gm.graph.nodes:
      if n.op != "placeholder":
        continue
      names.append(f"\"{n.target}\"")
    return names

  def __call__(self, *args):
    if not self._compiled:
      self._compiled = True
      clname = f"Module{self._inst_id}"
      with open(f"{clname}.scala", "w", encoding="utf-8") as f:
        self._emit(f, clname)

    self._call_id += 1
    call_id = self._call_id

    for i, arg in enumerate(args):
      with open(f"data{self._inst_id}_{self._call_id}_arg{i}.csv", "w", encoding="utf-8") as f:
        _tensor_to_matrix_csv(arg, f)

    result = self.gm(*args)

    with open(f"data{self._inst_id}_{self._call_id}_result.csv", "w", encoding="utf-8") as f:
      _tensor_to_matrix_csv(result, f)

    return result

  def _emit(self, f, clname):
    print("// This file is autogenerated", file=f)
    print("import backend.hdl.HDLProject", file=f)
    print("import core._", file=f)
    print("import algo._", file=f)
    print("import scala.io.Source", file=f)
    print(file=f)

    args = self._get_input_names()
    print("object", clname, "{", file=f)
    print(file=f)
    print("  def main(args: Array[String]): Unit = {", file=f)
    print("    HDLProject(\"boom\", _synth_ir()).simulateWithHostRam(Predef.Map(", file=f)
    for i, arg in enumerate(args):
      print("     ", arg, "-> _read_csv(args(", i ,"))", ",", file=f)
    print("      \"result\" -> _read_csv(args.last)", file=f)
    print("    )).assertCorrect.print()", file=f)
    print("  }", file=f)
    print(file=f)
    print("  def _synth_ir(): Expr = {", file=f)
    self._emit_body(f)
    print("  }", file=f)
    print(file=f)
    print("  def _read_csv(fname: String): Seq[Seq[Int]] = {", file=f)
    print("    val f = Source.fromFile(fname)", file=f)
    print("    val s = f.getLines().map(_.split(\",\").map(_.toInt).toList).toList", file=f)
    print("    f.close()", file=f)
    print("    s", file=f)
    print("  }", file=f)
    print(file=f)
    print("  // macros...", file=f)
    print("  def _idotp(ty: IntTypeT, x: Expr, y: Expr, acc: Option[Expr]=None): Expr = {", file=f)
    print("    val e1 = algo.TruncInteger(algo.Fold(algo.Add2.asFunction(),", file=f)
    print("      algo.Map({", file=f)
    print("        val _0 = core.ParamDef()", file=f)
    print("        algo.AlgoLambda(Seq(_0), algo.TruncInteger(algo.Mul(core.ParamUse(_0)), ty.width))", file=f)
    print("      }, algo.Zip(algo.Tuple(x, y)))), ty.width)", file=f)
    print("    val e2 = acc match {", file=f)
    print("      case None => e1", file=f)
    print("      case Some(e) => algo.TruncInteger(algo.Add2(e1, algo.TruncInteger(e, ty.width)), ty.width)", file=f)
    print("    }", file=f)
    print("    ty match {", file=f)
    print("      case SignedIntType(_) => algo.Sub(algo.Tuple(e2, algo.ConstantInteger(0)))", file=f)
    print("      case IntType(_) => e2", file=f)
    print("      case _ => ???", file=f)
    print("    }", file=f)
    print("  }", file=f)
    print(file=f)
    print("  def _iredsum(ty: IntTypeT, x: Expr): Expr = {", file=f)
    print("    val e1 = algo.TruncInteger(algo.Fold(algo.Add2.asFunction(),", file=f)
    print("      algo.Map(algo.TruncInteger.asFunction(Seq(None), Seq(ty.width)), x)), ty.width)", file=f)
    print("    ty match {", file=f)
    print("      case SignedIntType(_) => algo.Sub(algo.Tuple(e1, algo.ConstantInteger(0)))", file=f)
    print("      case IntType(_) => e1", file=f)
    print("      case _ => ???", file=f)
    print("    }", file=f)
    print("  }", file=f)
    print(file=f)
    print("  def _iredmax(ty: IntTypeT, x: Expr): Expr = {", file=f)
    print("    def signconv(e: Expr): Expr = ty match {", file=f)
    print("      case SignedIntType(_) => algo.Sub(algo.Tuple(e, algo.ConstantInteger(0)))", file=f)
    print("      case IntType(_) => e", file=f)
    print("      case _ => ???", file=f)
    print("    }", file=f)
    print("    signconv(algo.Fold({", file=f)
    print("      val _0 = core.ParamDef()", file=f)
    print("      val _1 = core.ParamDef()", file=f)
    print("      algo.AlgoLambda(Seq(_0, _1),", file=f)
    print("        algo.Max2(", file=f)
    print("          signconv(TruncInteger(ParamUse(_0), ty.width)),", file=f)
    print("          signconv(TruncInteger(ParamUse(_1), ty.width))))", file=f)
    print("    }, x))", file=f)
    print("  }", file=f)
    print(file=f)
    print("  def _add32_mm8(acc: Expr, lhs: Expr, rhs: Expr): Expr = {", file=f)
    print("    val seq = algo.SeqType(algo.SignedIntType(8), core.ArithTypeVar())", file=f)
    print("    val mm = algo.Map({", file=f)
    print("      val _0 = core.ParamDef(seq)", file=f)
    print("      algo.AlgoLambda(Seq(_0),", file=f)
    print("        algo.Map({", file=f)
    print("          val _1 = core.ParamDef(seq)", file=f)
    print("          algo.AlgoLambda(Seq(_1),", file=f)
    print("            algo.Sub(algo.Tuple(algo.TruncInteger(", file=f)
    print("              algo.Fold(Add2.asFunction(),", file=f)
    print("                algo.Map({", file=f)
    print("                  val _0 = core.ParamDef(algo.TupleType(algo.SignedIntType(8), algo.SignedIntType(8)))", file=f)
    print("                  algo.AlgoLambda(Seq(_0),", file=f)
    print("                    algo.TruncInteger(algo.Add2(", file=f)
    print("                      algo.ConstantInteger(0, Some(algo.SignedIntType(32))),", file=f)
    print("                      algo.Mul(core.ParamUse(_0))", file=f)
    print("                    ), 32)", file=f)
    print("                  )", file=f)
    print("                }, algo.Zip(algo.Tuple(core.ParamUse(_0), core.ParamUse(_1))))", file=f)
    print("              ),", file=f)
    print("            32), algo.ConstantInteger(0)))", file=f)
    print("          )", file=f)
    print("        }, rhs))", file=f)
    print("    }, lhs)", file=f)
    print("    algo.Map({", file=f)
    print("      val _0 = core.ParamDef()", file=f)
    print("      algo.AlgoLambda(Seq(_0),", file=f)
    print("        algo.Map({", file=f)
    print("          val _0 = core.ParamDef()", file=f)
    print("          algo.AlgoLambda(Seq(_0),", file=f)
    print("            algo.Sub(algo.Tuple(algo.TruncInteger(", file=f)
    print("              algo.Add(core.ParamUse(_0)),", file=f)
    print("              32", file=f)
    print("            ), algo.ConstantInteger(0)))", file=f)
    print("          )", file=f)
    print("        }, algo.Zip(algo.Tuple(core.ParamUse(_0), acc)))", file=f)
    print("      )", file=f)
    print("    }, mm)", file=f)
    print("  }", file=f)
    print("}", file=f)

  def _emit_body(self, f):
    for n in self.gm.graph.nodes:
      # input (placeholder) and output nodes must be 2D in SHIR.
      match n.op:
        case "placeholder":
          typ = shir_type.get_element_type(n)
          shape = n.meta.get("val").shape
          dims = len(shape)

          outer = inner = 1
          if dims == 1:
            inner = shape[0]
          elif dims == 2:
            [outer, inner] = shape
          elif dims > 2:
            outer = shape[0]
            inner = reduce(lambda x, y: x * y, shape[1:])

          v = f"algo.Input(\"{n.target}\", {typ.name()}, {inner}, {outer})"
          if dims != 2:
            v = f"algo.Join({v})"
          if dims < 1:
            v = f"core.Conversion({v}, {typ.name()})"
          if dims > 2:
            # the tuple splat usage here is safe because shape always has at
            # least three elements, so the trailing comma case never happens.
            v = f"algo.Join(algo.SplitAll({v}, Seq{(*shape,)}))"

          f.write(f"    val {n.name} = core.TypeChecker.check({v})\n")

        case "output":
          [retv] = n.args
          assert isinstance(retv, torch.fx.Node), "Only single fx node output is allowed"

          shape = retv.meta.get("val").shape
          dims = len(shape)
          v = retv.name

          if dims < 1:
            v = f"algo.Repeat({v}, 1)"
          if dims < 2:
            v = f"algo.Repeat({v}, 1)"
          if dims > 2:
            # avoid using Split(X, shape[0], false) since it causes problems
            inner_sz = reduce(lambda x, y: x * y, shape[1:])
            v = f"algo.Split(algo.JoinAll({v}), {inner_sz})"

          f.write(f"    return core.TypeChecker.check({v})\n")

        case "call_function":
          obj = shir_lowering.fetch_lowering(n.target)
          v = obj.lower(*n.args, **n.kwargs)
          f.write(f"    val {n.name} = core.TypeChecker.check({v})\n")

        case _:
          assert False, "Unhandled fx node type when emitting"

class SHIROperatorSupport(OperatorSupport):
  def is_node_supported(self, submodules, n: torch.fx.Node) -> bool:
    if n.op not in CALLABLE_NODE_OPS:
      return False

    try:
      # clearly if shir can't represent this type, then we can't process it.
      # e.g. max_pool2d returns a tuple?
      if not shir_type.has_shir_type(n):
        return False

      obj = shir_lowering.fetch_lowering(n.target)
      return obj.supports(*n.args, **n.kwargs)
    except:
      return False

def apply_shir_ops(gm: torch.fx.GraphModule):
  # the supported operators would be punted off into submodules,
  # so only look for those and compile those
  for n in gm.graph.nodes:
    if n.op == "call_module":
      assert not n.kwargs
      submod = gm.get_submodule(n.target)
      gm.delete_submodule(n.target)
      gm.add_submodule(n.target, SHIRGraphModule(submod))

def compiler(gm: torch.fx.GraphModule, example_inputs: list[torch.Tensor]):
  # raw ops -> quantized rewrite -> core aten + prims
  #   -> partition -> shir

  def lowering(gm, example_inputs):
    # gm.print_readable()
    supported_ops = SHIROperatorSupport()
    partitioner = CapabilityBasedPartitioner(gm, supported_ops,
                                             allows_single_node_partition=True)
    partitions = partitioner.propose_partitions()
    fused_graph = partitioner.fuse_partitions(partitions)
    apply_shir_ops(fused_graph)

    return make_boxed_func(fused_graph.forward)

  rewrite_pattern.rewrite_quantized_ops(gm)

  augdecomps = core_aten_decompositions()
  augdecomps.update(_decomps)

  f = aot_module_simplified(gm, example_inputs,
                            decompositions=augdecomps,
                            fw_compiler=lowering)
  return f
