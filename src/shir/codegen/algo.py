"""
The old way of doing project generation, which involves emitting
algo + algo.torch level SHIR primitives.
"""

from shir import types, layout, config, bit_utils
from shir.codegen.project import SHIRProject
import torch
from torch.fx import GraphModule, Node
from pathlib import Path
import shir.codegen.algo_lowering

def fetch_lowering(key):
  return shir.codegen.algo_lowering.fetch_lowering(key)

def has_many_uses(node: Node) -> bool:
  user_count = len(node.users)
  if user_count > 1:
    return True

  used = False
  for user in node.users:
    # each user may have multiple occurrences / uses of a single node
    for n in user.all_input_nodes:
      if n != node:
        continue
      if used:
        return True
      used = True

  return False

class Project(SHIRProject):
  def __init__(self, clname: str, output_dir: Path):
    super(Project, self).__init__(clname, output_dir)

  def emit_source(self, gm, host_mapping):
    with (self.output_dir / f"{self.clname}.scala").open("w", encoding="utf-8") as f:
      print("// This file is autogenerated", file=f)
      print("import core._", file=f)
      print("import algo._", file=f)
      print("import java.nio.file.Paths", file=f)

      print(file=f)
      print("object", self.clname, "extends support.GeneratedModel {", file=f)

      print(file=f)
      print("  override val name: String = \"", self.clname, "\"", sep="", file=f)

      print(file=f)
      print("  def main(args: Array[String]): Unit = support.Util.drive(this, args)", file=f)

      # determine the buffering strategy here.
      # it's needed because doing so might uncover extra host mappings
      # (due to buffering on host).
      buffer_strategy = self._determine_buffering(gm, host_mapping)

      print(file=f)
      self._emit_method_load_data(f, gm, host_mapping)

      print(file=f)
      self._emit_method_extra_rewrites(f, gm, buffer_strategy, host_mapping)

      print(file=f)
      self._emit_method_generate_ir(f, gm, host_mapping)

      print("}", file=f)

  def _determine_buffering(self, gm, host_mapping):
    buffer_id = 0
    buffer_strategy = {}
    for n in gm.graph.nodes:
      if n.op != "call_function":
        continue

      obj = fetch_lowering(n.target)
      try:
        if callable(obj.should_buffer):
          hint = obj.should_buffer(*n.args, **n.kwargs)
          for node, flag in hint.items():
            original_flag = buffer_strategy.get(node, None)
            buffer_strategy[node] = layout.merge_buffer_info(original_flag, flag)

            # right now, assume that we really want it buffered on host.
            # so add a new entry if it wasn't there already
            if node not in host_mapping:
              host_mapping[node] = (f"buffer{buffer_id}_tag", types.get_element_type(node))
              buffer_id += 1
      except:
        pass
    return buffer_strategy

  def _emit_method_load_data(self, f, gm, host_mapping):
    print("  override def loadData(folder: String): Predef.Map[String, Seq[Seq[Int]]] = Predef.Map(", file=f)

    output_node = None
    for n in gm.graph.nodes:
      if n.op == "output":
        assert output_node is None, f"Multi-output node not supported"
        assert isinstance(n.args[0], Node), "Multi-valued output node not supported"
        output_node = n.args[0].meta.get("val")
        continue

      # it is either a placeholder or a intermediate node.
      # in either case, consult the host_mapping table.
      if n in host_mapping:
        host_id = host_mapping[n][0]

        # if it is an input, read the values from csv files.
        # otherwise, just create a bunch of 0's (like in the result case)
        if n.op == "placeholder":
          print(
            "    \"", host_id, "\" -> support.Util.readIntCSV(Paths.get(folder, \"",
            host_id, ".csv\").toFile()),",
            sep="", file=f
          )
        else:
          (outer, inner) = layout.reshape_size_to_matrix(n.meta.get("val").shape)
          print(
            "    \"", host_id, "\" -> new support.UniformSeq(new support.UniformSeq(0, ",
            inner, "), ", outer, "),",
            sep="", file=f
          )

    # for the result, we still need to give it some dummy value for simulation
    # to determine the RAM size.
    (outer, inner) = layout.reshape_size_to_matrix(output_node.shape)
    print(
      "    \"result\" -> new support.UniformSeq(new support.UniformSeq(0, ",
      inner, "), ", outer, ")",
      sep="", file=f
    )

    print("  )", file=f)

  def _emit_method_extra_rewrites(self, f, gm, buffer_strategy, host_mapping):
    print("  override def extraRewrites(): Seq[(core.compile.CompilerPhase, core.rewrite.RewriteStep)] = {", file=f)
    print("    import core.compile.CompilerPhase", file=f)
    print("    import core.rewrite.{RewriteAll, RewriteStep, RewriteTargeted}", file=f)
    print("    import backend.hdl.arch.{ArchCompiler, MapCompiler}", file=f)
    print("    import backend.hdl.arch.device.DeviceSpecificCompiler", file=f)
    print("    import backend.hdl.arch.tiling.PaddingCompiler", file=f)
    print("    import backend.hdl.arch.rewrite.{InputBufferingRules, ParallelizeDotProductRules}", file=f)
    print("    import backend.hdl.arch.mem.MemFunctionsCompiler", file=f)
    print("    Seq(", file=f)

    print("(ArchCompiler.phaseBefore, RewriteStep(RewriteAll(), Seq(", file=f)
    print("  algo.torch.rewrite.Rules.padInputToCacheline(512)))),", file=f)

    print("(ArchCompiler.phaseAfter, RewriteStep(RewriteAll(), Seq(", file=f)
    print("  ParallelizeDotProductRules.targetTorchConvKernel,", file=f)
    print("  ParallelizeDotProductRules.targetTorchPoolKernel,", file=f)
    print("  ParallelizeDotProductRules.targetZipmapChannel))),", file=f)

    # and we emit the rewrite rules based on the collected hints.
    #
    # XXX:
    # the buffering rewrites MUST be applied outside in. so for a graph
    # of { tmp = input1 * input2; result = tmp * input3 }, tmp must be
    # buffered before input1 and input2.
    #
    # (and the current implementation is obeys that by walking the
    # dependency graph backwards)
    for n in reversed(gm.graph.nodes):
      if n not in host_mapping or n not in buffer_strategy:
        continue

      host_id, real_typ = host_mapping[n]
      strat = buffer_strategy[n]
      if strat is None:
        continue

      # buffering is based on cachelines, so estimate it.
      # (the rewrite will crash later if there's a mismatch...)
      _, lines = layout.guess_line_layout(n.meta.get("val").shape, real_typ)
      print("(ArchCompiler.phaseAfter, RewriteStep(RewriteAll(), Seq(", end='', file=f)

      match strat:
        case layout.BufferMatrix(u):
          lines = u if u else lines
          print("InputBufferingRules.bufferInputMatrix(\"", host_id, "\", ", lines, ")", sep='', end='', file=f)

        case layout.BufferRow(u):
          lines = u if u else lines
          print("InputBufferingRules.bufferInputRow(\"", host_id, "\", ", lines, ")", sep='', end='', file=f)

      print("))),", file=f)

    # add other operation specific rewrites
    for n in gm.graph.nodes:
      if n.op != "call_function":
        continue

      obj = fetch_lowering(n.target)
      try:
        if callable(obj.should_rewrite):
          hint = obj.should_rewrite(*n.args, **n.kwargs)
          if hint is not None:
            print(hint, ",", sep="", file=f)
      except:
        pass

    # try to increase the number of parallel read requests
    print(f"(PaddingCompiler.phaseBefore, RewriteStep(RewriteAll(), Seq(InputBufferingRules.increaseParallelReadRequests(64)))),", file=f)

    # double buffer every input
    print(f"(MemFunctionsCompiler.phaseAfter, RewriteStep(RewriteAll(), InputBufferingRules.readDoubleBuffering)),", file=f)

    print("    )", file=f)
    print("  }", file=f)

  def _emit_method_generate_ir(self, f, gm, host_mapping):
    print("  override def generateIR(): Expr = {", file=f)

    lets_needed = 0
    for n in gm.graph.nodes:
      # assume every node that has many uses needs to be let-bound,
      # which is definitely the case for tensors (which are SeqType's)
      many_uses = has_many_uses(n)
      if many_uses:
        lets_needed += 1

      # furthermore, input (placeholder) and output nodes must be 2D in SHIR.
      if n.op == "placeholder":
        # since these are all tensors, we would have gotten type and shape
        # annotations on all of them.
        #
        # the actual SHIR type may be different from the tensors' metadata:
        # -  the signedness has to match
        # -  the width may be narrower than the annotation
        #
        # the users can assume input tensors already satisfy these properties
        # and are expected to do the corresponding extension if necessary.

        host_id, real_typ = host_mapping[n]
        ndim = n.meta.get("val").ndim
        shape = [*n.meta.get("val").shape]

        transpose = None
        if config.USE_CHANNEL_LAST and ndim > 2:
          shape = [shape[0]] + shape[2:] + [shape[1]]
          transpose = [*range(1, ndim - 1), 0, ndim - 1]

        node = (
          f"algo.torch.Input({real_typ.name()}, \"{host_id}\","
          f" Seq({', '.join((str(d) for d in shape))}))"
        )
        if transpose is not None:
          node = f"algo.TransposeND({node}, Seq({', '.join((str(d) for d in transpose))}))"

        if many_uses:
          print(
            "  { val _init = core.TypeChecker.check(", node, ")\n",
            "    val _param = core.ParamDef(_init.t)\n",
            "    core.Let(_param,\n",
            "  { val ", a.name, " = core.ParamUse(_param)",
            sep="", file=f
          )
        else:
          print(
            "    val ", n.name, " = core.TypeChecker.check(", node, ")",
            sep="", file=f
          )

      elif n.op == "output":
        # sometimes, due to unfortunate graph slicing, we may end up with
        # multiple outputs, which we cannot handle
        [retv] = n.args
        assert isinstance(retv, Node), "Only single node output is allowed"
        assert not many_uses  # not sure what this failing would mean...

        annot_typ = types.get_element_type(retv)
        dims = retv.meta.get("val").ndim
        v = retv.name

        if dims < 1:
          v = f"algo.Repeat({v}, 1)"
        if dims < 2:
          v = f"algo.Repeat({v}, 1)"
        if dims > 2:
          if config.USE_CHANNEL_LAST:
            transpose = [dims - 2, *range(0, dims - 2), dims - 1]
            v = f"algo.TransposeND({v}, Seq({', '.join((str(d) for d in transpose))}))"
          if dims == 4:
            v = f"algo.Join(algo.torch.Flatten({v}, 2, 3))"
          else:
            v = f"algo.torch.Flatten({v}, 1, {dims - 1})"

        print(
          "    core.TypeChecker.check(algo.Map(2,",
          " algo.ResizeInteger.asFunction(types = Seq(", annot_typ.bits, ")),",
          " ", v, "))",
          sep="", file=f
        )

      elif n.op == "call_function":
        obj = fetch_lowering(n.target)
        expr = obj.lower(*n.args, **n.kwargs)
        if n in host_mapping:
          # add buffering if necessary.
          expr = f"algo.BufferHost({expr}, core.TextType(\"{host_mapping[n][0]}\"))"

        if many_uses:
          print(
            "    val _init = core.TypeChecker.check(", expr, ")\n",
            "    val _param = core.ParamDef(_init.t)\n",
            "    core.Let(_param,\n",
            "  { val ", n.name, " = core.ParamUse(_param)",
            sep="", file=f
          )
        else:
          print(
            "    val ", n.name, " = core.TypeChecker.check(", expr, ")",
            sep="", file=f
          )

      else:
        assert False, "Unhandled fx node type when emitting"

    for _ in range(lets_needed):
      print("  }, _init)", file=f)

    print("  }", file=f)

