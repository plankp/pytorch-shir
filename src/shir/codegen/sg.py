from shir import types, layout, config, bit_utils
from shir.codegen.project import SHIRProject
import torch
from torch.fx import GraphModule, Node
from typing import Tuple, List
from pathlib import Path
import shir.codegen.sg_lowering

def fetch_lowering(key):
  return shir.codegen.sg_lowering.fetch_lowering(key)

def inverse_transpose(t: List[int]) -> List[int]:
  return [x[0] for x in sorted(enumerate(t), key=lambda x: x[1])]

class Project(SHIRProject):
  def __init__(self, clname: str, output_dir: Path):
    super(Project, self).__init__(clname, output_dir)

  def emit_source(self, gm, host_mapping):
    with (self.output_dir / f"{self.clname}.scala").open("w", encoding="utf-8") as f:
      print("// This file is autogenerated", file=f)
      print("package backend.hdl.arch", file=f)   # because we emit Arch ops directly
      print("import core._", file=f)
      print("import core.compile.CompilerPhase", file=f)
      print("import java.nio.file.Paths", file=f)

      print(file=f)
      print("object", self.clname, "extends support.GeneratedModel {", file=f)

      print(file=f)
      print("  override val name: String = \"", self.clname, "\"", sep="", file=f)

      print(file=f)
      print("  def main(args: Array[String]): Unit = support.Util.drive(this, args)", file=f)

      print(file=f)
      print("  override def compilerPhase(): CompilerPhase = mem.MemLayoutCompiler.phaseBefore", file=f)

      print(file=f)
      print("  override def generateIR(): Expr = {", file=f)
      print("    import eqsat.solver._", file=f)
      print("    val e1 = TypeChecker.check(this.generateSGIR())", file=f)
      print("    val e2 = sg.Preprocessor(e1)", file=f)
      print("    val mp = sg.ProblemGenerator(e2)", file=f)
      print("    val sv = IterativeZ3Py().solve(mp).get", file=f)
      print("    val e3 = sg.Postprocessor(sg.SolutionInstantiator(sv, e2))", file=f)
      print("    e3", file=f)
      print("  }", file=f)

      print(file=f)
      self._emit_method_load_data(f, gm, host_mapping)

      print(file=f)
      self._emit_method_generate_ir(f, gm, host_mapping)

      print("}", file=f)


  def _emit_method_load_data(self, f, gm, host_mapping):
    print("  override def loadData(folder: String): Predef.Map[String, Seq[Seq[Int]]] = Predef.Map(", file=f)

    output_node = None
    for n in gm.graph.nodes:
      if n.op == "output":
        assert output_node is None, f"Multi-output node not supported"
        assert isinstance(n.args[0], Node), "Multi-valued output node not supported"
        output_node = n.args[0].meta.get("val")
        continue

      # it is either a placeholder or a intermediate node.
      # in either case, consult the host_mapping table.
      if n in host_mapping:
        host_id = host_mapping[n][0]

        # if it is an input, read the values from csv files.
        # otherwise, just create a bunch of 0's (like in the result case)
        if n.op == "placeholder":
          print(
            "    \"", host_id, "\" -> support.Util.readIntCSV(Paths.get(folder, \"",
            host_id, ".csv\").toFile()),",
            sep="", file=f
          )
        else:
          (outer, inner) = layout.reshape_size_to_matrix(n.meta.get("val").shape)
          print(
            "    \"", host_id, "\" -> new support.UniformSeq(new support.UniformSeq(0, ",
            inner, "), ", outer, "),",
            sep="", file=f
          )

    # for the result, we still need to give it some dummy value for simulation
    # to determine the RAM size.
    (outer, inner) = layout.reshape_size_to_matrix(output_node.shape)
    print(
      "    \"result\" -> new support.UniformSeq(new support.UniformSeq(0, ",
      inner, "), ", outer, ")",
      sep="", file=f
    )

    print("  )", file=f)

  def _emit_method_generate_ir(self, f, gm, host_mapping):
    print("  def generateSGIR(): Expr = {", file=f)
    print("    import backend.hdl._", file=f)

    for n in gm.graph.nodes:
      if n.op == "placeholder":
        host_id, real_typ = host_mapping[n]
        ndim = n.meta.get("val").ndim
        shape = [*n.meta.get("val").shape]

        transpose, (h, w) = layout.pack_host_shape(shape)
        shape = [shape[x] for x in transpose]
        itr = inverse_transpose(transpose)

        ttyp = f"sg.TensorType(Seq({h}, {w}), {real_typ.name()})"
        node = f"sg.SolverGuidedInput(\"{host_id}\", {ttyp})"
        node = f"sg.SolverGuidedRebalance(sg.SolverGuidedReshape({node}, Seq({', '.join((str(d) for d in shape))})))"
        node = f"sg.SolverGuidedPermute({node}, Seq({', '.join((str(d) for d in itr))}))"

        print("    val ", n.name, " = TypeChecker.check(", node, ")", sep="", file=f)

      elif n.op == "output":
        [retv] = n.args
        assert isinstance(retv, Node), "Only single node output is allowed"

        annot_typ = types.get_element_type(retv)
        ndim = retv.meta.get("val").ndim
        shape = retv.meta.get("val").shape

        transpose, (h, w) = layout.pack_host_shape(shape)
        conv = f"ResizeInteger.asFunction(types = Seq({annot_typ.bits}))"
        node = f"sg.SolverGuidedMap({ndim})({conv}, {retv.name})"
        node = f"sg.SolverGuidedPermute({node}, Seq({', '.join((str(d) for d in transpose))}))"
        node = f"sg.SolverGuidedReshape(sg.SolverGuidedRebalance({node}), Seq({h}, {w}))"
        print("    TypeChecker.check(sg.SolverGuidedOutput(", node, "))", sep="", file=f)

      elif n.op == "call_function":
        obj = fetch_lowering(n.target)
        expr = obj.lower(*n.args, **n.kwargs)
        print("    val ", n.name, " = TypeChecker.check(sg.SolverGuidedRebalance(", expr, "))", sep="", file=f)

      else:
        assert False, "Unhandled fx node type when emitting"

    print("  }", file=f)

